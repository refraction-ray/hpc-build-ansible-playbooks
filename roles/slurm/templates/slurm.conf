# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine={{ ctldhost[0] }}
#ControlAddr=
BackupController={{ ctldhost[1] }}
#BackupAddr=
# 
AuthType=auth/munge
#CheckpointType=checkpoint/none 
CryptoType=crypto/munge
#DisableRootJobs=NO 
#EnforcePartLimits=NO 
#Epilog=
#EpilogSlurmctld= 
#FirstJobId=1 
#MaxJobId=999999 
GresTypes=gpu 
#GroupUpdateForce=0 
#GroupUpdateTime=600 
#JobCheckpointDir=/var/slurm/checkpoint 
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0 
#JobRequeue=1 
#JobSubmitPlugins=1 
#KillOnBadExit=0 
#LaunchType=launch/slurm 
#Licenses=foo*4,bar 
MailProg=/usr/bin/smail.sh 
MaxJobCount=1000 
#MaxStepCount=40000 
#MaxTasksPerNode=128 
MpiDefault=none
#MpiParams=ports=#-# 
#PluginDir= 
#PlugStackConfig= 
#PrivateData=jobs 
ProctrackType=proctrack/cgroup
#Prolog=
PrologFlags=contain # for pam module 
#PrologSlurmctld= 
#PropagatePrioProcess=0 
PropagateResourceLimits=NONE 
#PropagateResourceLimitsExcept= 
#RebootProgram= 
ReturnToService=1
#SallocDefaultCommand= 
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/tmp/slurmd
SlurmUser={{ slurm_user }}
#SlurmdUser=root 
#SrunEpilog=
#SrunProlog=
StateSaveLocation={{ slurm_spool_path }}
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/affinity
TaskPluginParam=Sched
#TaskProlog=
#TopologyPlugin=topology/tree 
#TmpFS=/tmp 
#TrackWCKey=no 
#TreeWidth= 
#UnkillableStepProgram= 
#UsePAM=0 
# 
# 
# TIMERS 
#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
#HealthCheckInterval=0 
#HealthCheckProgram= 
InactiveLimit=0
KillWait=30
#MessageTimeout=10 
#ResvOverRun=0 
MinJobAge=300
#OverTimeLimit=0 
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60 
#VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
#DefMemPerCPU=0 
FastSchedule=0
#MaxMemPerCPU=0 
#SchedulerTimeSlice=30 
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core
# 
# 
# JOB PRIORITY 
#PriorityFlags= 
PriorityType=priority/multifactor 
PriorityDecayHalfLife=0 
#PriorityCalcPeriod= 
PriorityFavorSmall=YES 
#PriorityMaxAge= 
PriorityUsageResetPeriod=YEARLY 
PriorityWeightAge=1000
PriorityWeightFairshare=300 
PriorityWeightJobSize=100 
#PriorityWeightPartition= 
PriorityWeightQOS=600 
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageEnforce=limits,qos 
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
AccountingStoragePort=6819
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ dbdhost }}
#AccountingStorageUser=
AccountingStorageTRES=gres/gpu,gres/gpu:RTX2080TI
AccountingStoreJobComment=YES
ClusterName={{ cluster_name }}
#DebugFlags= 
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none 
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=3
#SlurmctldLogFile=
SlurmdDebug=3
#SlurmdLogFile=
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
#SuspendProgram= 
#ResumeProgram= 
#SuspendTimeout= 
#ResumeTimeout= 
#ResumeRate= 
#SuspendExcNodes= 
#SuspendExcParts= 
#SuspendRate= 
#SuspendTime= 
# 
# 
# COMPUTE NODES 

{% for h in groups['cn'] %}
{% if h in groups['gn'] %}
NodeName={{ h }} State=UNKNOWN Weight=20 CoresPerSocket={{ hostvars[h]['corespersocket']|default('14') }} Sockets=2 ThreadsPerCore=2 RealMemory={{ hostvars[h]['memory']|default('128000') }} Gres=gpu:{{ hostvars[h]['gputype']|default('RTX2080TI') }}:{{ hostvars[h]['gpuno']|default("2") }}
{% else %}
NodeName={{ h }} State=UNKNOWN Weight=10 CoresPerSocket={{ hostvars[h]['corespersocket']|default('14') }} Sockets=2 ThreadsPerCore=2 RealMemory={{ hostvars[h]['memory']|default('128000') }}
{% endif %}
{% endfor %} 
# NodeName={{ master_name }} State=UNKNOWN Weight=30 CoresPerSocket=14 Sockets=2 ThreadsPerCore=2 RealMemory=128000

PartitionName=general MaxTime=Infinite Nodes={% for h in groups['general'] %}{{h+"," if not loop.last else h}}{% endfor %} PriorityJobFactor=5000 Default=YES State=UP
PartitionName=hyper MaxTime=Infinite Nodes={% for h in groups['hyper'] %}{{h+"," if not loop.last else h}}{% endfor %} PriorityJobFactor=5000 Default=NO State=UP
PartitionName=debug MaxTime=00:30:00 Nodes=ALL PriorityJobFactor=50000 Default=NO State=UP
{% if groups['cn']|intersect(groups['gn']) %}
PartitionName=gpu MaxTime=Infinite Nodes={% for h in groups['cn']|intersect(groups['gn'])  %}{{h+"," if not loop.last else h}}{% endfor %}   Default=No AllowAccounts=ubuntu TRESBillingWeights="CPU=1.0,GRES/gpu=3.0"
{% endif %}
